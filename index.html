<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation and Recognition</title>
<style type="text/css">
<!--  body {-->
<!--  margin: 0  auto；-->
<!--  padding:10px 30px;-->
<!--  background: #fff;-->
<!--  color: #111;-->
<!--  font-size: 15px;-->
<!--  font-family: "Times New Roman", serif;-->
<!--  font-weight: 400;-->
<!--  line-height: 1.8;-->
<!--  -webkit-font-smoothing: antialiased;-->
<!--  }-->
  audio {
  width: 75%;
  height:30px;
  }
  img {
  width:100%;
  }
  video {
  width:75%;
  }
  html{
  height:100%;
  }
  body{
  margin: 0  auto；
  padding:10px 30px;
  background: #fff;
  color: #111;
  height:100%;
  font-size: 17px;
  font-family: "Times New Roman", serif;
  font-weight: 400;
  line-height: 1.8;
  overflow-x: hidden;
  -webkit-font-smoothing: antialiased;
  }
</style>
</head>
<div style="border: none; width:80%; margin: 0 auto;">
<body>
    <h2 align="center">Speech Enhancement System Compression Using
Parameter Tying and Mixed Precision Quantization</h2>
    <p style="line-height:100%" align="center"><b>Authors:</b> </p>
    <p style="line-height:100%" align="justify"> <b>Abstract: </b> Speech enhancement systems are crucial components in many speech processing tasks, such as automatic speech recognition (ASR). Their practical deployment on devices requires their memory footprints and computation costs to be aggressively reduced. To this end, a novel KL divergence based parameter sharing approach is proposed for state-of-the-art temporal convolutional networks (TCNs) based audio-visual multi-channel speech separation systems featuring mask-based MVDR beamforming. Experiments are conducted on two tasks using overlapped speech simulated from the English Oxford LRS2 and Cantonese CI-AVSR datasets. The efficacy of the proposed TCN parameter tying approach is extensively shown when used as a standalone model architecture compression technique in comparison to low-rank factorization and penalized neural architecture search, as well as when combined with low-rank factorization and mixed precision low-bit precision quantization approaches. An overall “lossless” model compression ratio of up to 18.54 times is obtained over the baseline uncompressed audio-visual multi-channel speech separation system while incurring no statistically significant ASR word or character error rate increase. Comparable speech enhancement performance measured using scale-invariant signal-to-noise ratio (SI-SNR) scores is also produced. The same “lossless” speech enhancement system compression ratio is retained after applying 5-bit mixed precision quantization to the Conformer ASR back-end.</p>
</body>
</div>
</html>
