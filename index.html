<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Speech Enhancement System Compression Using
Parameter Tying and Mixed Precision Quantization</title>
<style type="text/css">
<!--  body {-->
<!--  margin: 0  auto；-->
<!--  padding:10px 30px;-->
<!--  background: #fff;-->
<!--  color: #111;-->
<!--  font-size: 15px;-->
<!--  font-family: "Times New Roman", serif;-->
<!--  font-weight: 400;-->
<!--  line-height: 1.8;-->
<!--  -webkit-font-smoothing: antialiased;-->
<!--  }-->
  audio {
  width: 75%;
  height:30px;
  }
  img {
  width:100%;
  }
  video {
  width:75%;
  }
  html{
  height:100%;
  }
  body{
  margin: 0  auto；
  padding:10px 30px;
  background: #fff;
  color: #111;
  height:100%;
  font-size: 17px;
  font-family: "Times New Roman", serif;
  font-weight: 400;
  line-height: 1.8;
  overflow-x: hidden;
  -webkit-font-smoothing: antialiased;
  }
</style>
</head>
<div style="border: none; width:80%; margin: 0 auto;">
<body>
    <h2 align="center">Speech Enhancement System Compression Using
Parameter Tying and Mixed Precision Quantization</h2>
    <p style="line-height:100%" align="center"><b>Authors:</b> </p>
    <p style="line-height:100%" align="justify"> <b>Abstract: </b> Speech enhancement systems are crucial components in many speech processing tasks, such as automatic speech recognition (ASR). Their practical deployment on devices requires their memory footprints and computation costs to be aggressively reduced. To this end, a novel KL divergence based parameter sharing approach is proposed for state-of-the-art temporal convolutional networks (TCNs) based audio-visual multi-channel speech separation systems featuring mask-based MVDR beamforming. Experiments are conducted on two tasks using overlapped speech simulated from the English Oxford LRS2 and Cantonese CI-AVSR datasets. The efficacy of the proposed TCN parameter tying approach is extensively shown when used as a standalone model architecture compression technique in comparison to low-rank factorization and penalized neural architecture search, as well as when combined with low-rank factorization and mixed precision low-bit precision quantization approaches. An overall “lossless” model compression ratio of up to 18.54 times is obtained over the baseline uncompressed audio-visual multi-channel speech separation system while incurring no statistically significant ASR word or character error rate increase. Comparable speech enhancement performance measured using scale-invariant signal-to-noise ratio (SI-SNR) scores is also produced. The same “lossless” speech enhancement system compression ratio is retained after applying 5-bit mixed precision quantization to the Conformer ASR back-end.</p>
    <a href="#sectionI">I. Audio-visual Multi-channel Mask-based MVDR Speech Separation Front-end</a><br>
    <a href="#sectionIII">III. TCN Model Architecture Compression</a><br>
    <a href="#sectionIII">III. Low-bit Precision Quantizatoin Of TCN</a><br>
    <a href="#sectionIII">IV. Experimental Setup and Results</a><br>
    <a href="#sectionIV">IV. Reference</a>
    <h3> <a name="sectionI"> I. Audio-visual Multi-channel Mask-based MVDR Speech Separation Front-end</a> </h3>
    <div style="border: none; width:60%; margin: 0 auto;">
    <img border="0" src="architectures.png" alt="architectures of speech enhancement front-end" height="10%">
    </div>
    <ul>
        <li> In the architecture-1, speech separaiton (based on the audio-visual mask-based MVDR [1]) followed by dereverberation (based on the audio-visual DNN-WPE or SpecM [2]).</li>
        <li> In the architecture-2, speech dereverberation (based on the audio-visual DNN-WPE or SpecM) followed by separation (based on the audio-visual mask-based MVDR).</li>
        <li> In the architecture-3, joint speech separation & dereverberation using the audio-visual mask-based WPD. </li>
        <li> The internal structural details of audio-visual mask-based MVDR, DNN-WPE, SpecM and mask-based WPD as well as the Visual Front-end modules can be seen in  Fig. 1 and Fig. 3 of the submitted paper.  </li>
    </ul>
</body>
</div>
</html>
